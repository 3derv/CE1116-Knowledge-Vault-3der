---
Fecha de creación: 2025-09-02 19:11
Fecha de Modificación: 2025-09-02 19:11
tags:
  - IA
Tema: inteligencia-artificial
---


## 📚 Idea/Concepto 
La tokenización es el proceso de convertir un texto en unidades llamadas tokens, que pueden ser palabras, subpalabras o caracteres. Estos tokens se transforman en índices numéricos que corresponden a un vocabulario definido, lo cual permite que los modelos de lenguaje procesen texto de manera eficientemente en rendimiento y costo. El  esquema de tokenización influye en la cantidad de información semántica que será cubierta por la ventana de contexto. Técnicas como Byte Pair Encoding (BPE) y WordPiece dividen palabras en fragmentos para manejar vocabularios grandes y palabras desconocidas. también hay algoritmos de decodificación de tokens

## 📌 Puntos Claves (Opcional)
- 

## 🔗 Connections
- [[ ]]

## 💡 Personal Insight (Opcional)
- 
## 🧾 Recursos (Opcional)
- 