---
Fecha de creaci贸n: 2025-09-02 19:09
Fecha de Modificaci贸n: 2025-09-02 19:09
tags:
  - IA
Tema: inteligencia-artificial
---


##  Idea/Concepto 
Los mecanismos de atenci贸n en transformadores es un proceso que permite al modelo enfocarse en las palabras m谩s relevantes del contexto, poseen dos vectores principales que son proyecciones lineales de los embeddings, el vector de querys (Q) y keys (k) que interact煤an entre ellas con producto punto escalado (normalizado) para medir la similitud sobre el cual se aplica la funci贸n softmax , luego en la matriz de valores (V) se suman ponderadamente estos valores para obtener los pesos de atenci贸n. Esta atenci贸n es se divide en m煤ltiples cabezas de atenci贸n que trabajan en paralelo, cada uno se enfoca en cierto tipo de relaciones en el texto y este usa una red feed forward que procesa cada token independientemente para profundizar el entendimiento sem谩ntico. Los embeddings tambi茅n incorporan codificaciones posicionales para incluir informaci贸n sobre el orden de los tokens

##  Puntos Claves (Opcional)
- 

##  Connections
- [[ ]]

##  Personal Insight (Opcional)
- 
## Ь Recursos (Opcional)
- 